Alex Xu

First we focus on a single server setup 
1. Users access the website through the domain name. Usually the DNS(Domain Name System) is a paid 
service hosted by a third party(Godaddy offers a DNS service as part of its suite of services).
DNS transforms human readable addresses into IP addresses. The browser sends the DNS query to a DNS resolver.
2. IP address is returned to the web browser
3. Once the IP address is obtained, HTTP requests are sent directly to the web server
4. The server sends HTML pages or JSON response for rendering

There can be 2 main sources, web apps and mobile apps
Web apps use a combination of server side languages to handle business logic and client side languages for 
presentation
JSON is used for api response format.


Vertical scaling has a hard limit and does not have failover and redundancy
In horizontal scaling, a load balancer is the best technique to ensure failover and redundancy

In order to support failover and redundancy for a single database, database replication is a simple idea 
In its simplest form, database replication uses a master slave architecture

The data is also replicated in multiple servers so that data loss doesn't happen

If the master database goes down, a slave database will be promoted to the master. A new slave database
will replace the old one for replication temporarily(even this may not be needed). In prod systems, 
promoting a new master is complicated as data in a slave database might not be up to date. We need to
run data recovery scripts to sync up the databases

There are some other methods like multi-master and circular replication.

In multi master architecture, all nodes have both read and write capabilities unlike where in single master
only the master does the write while the slaves are read only
Conflict resolution is done by LWW(Last Write Wins). The system maintains different versions of the data 
to check which version is the authoritative one. Also in this specific business rules govern how different 
conflicts will be handled

Multimaster systems are common in cloud applications where availability and scalability are key 

Circular replication is a replication system that can be used in multi master architecture among others. 
Here if we think of nodes in a circle, data flows in a loop and data for node A is duplicated in node B, node B to node C, etc.
Even here we can use LWW for resolving conflicts 
 
Once the cache is full we can use any kind of cache eviction like LRU, LFU, etc.

Since a CDN loads static content from a cache. Thus we can also use Dynamic Content Caching in CDNs 
to make it even faster

When caching in CDNs we must also configure how to cope with CDN failure. Most CDNs offer automatic 
failover to origin. But that can make it incredibly slow, so there are different approaches like Multi CDN strategy, Origin Shields, Edge caching with redundancy, local caching,etc.


Stateless web tier-> Is a design pattern where each request made by the client to the server is independent 
and self contained, and the server doesn't store any prior information of the client 
REST api is a stateless architecture. Every API request is independent and the server does not maintain any information between requests.
In a serverless model like AWS Lambda, each function is stateless and processes a single request without maintaining user data

Challenges-> Since the server doesn't remember previous interactions, clients may need to store some information 
like user sessions or authentication tokens 

On the other hand, the pattern where we store metadata from client requests is called stateful architecture.


A stateless system is simpler, more robust and scalable.


We can use services like geoDNS to geographically route requests.

This allows us to have multi data centers for each geo location.

For further scaling, we decouple different components of the system so that they can be scaled efficiently.

Messaging queue is one step towards decoupling the queueing process to make it scalable

All_Local_Code_and_Notes\System Design images\1st.png




Stackoverflow and Spotify used only 1 database. The overall cost of vertical scaling is high as single high 
performance servers are much more expensive

Horizontal scaling is also known as sharding
Sharding separates large databases into small easily managed parts called shards

The most important factor to consider when implementing a sharding strategy is choosing a sharding key


New challenges in sharding:
1. Resharding data is needed when a single shard cannot handle /hold more data due to rapid growth.
Certain shards might experience shard exhaustion faster due to uneven data distribution.
Consistent hashing is a common technique used to solve this problem.
2. Celebrity problem: What instagram faced with JB uploads.
3. Join and Denormalization: Once a DB has been sharded across multiple servers, it is hard to perform Join
operations across database shards. A common workaround is to denormalize the database so that the queries can 
be performed in a single table.

All_Local_Code_and_Notes\System Design images\2nd.png


Scaling a system is an iterative process. Many fine tuning strategies will be needed to make it beyond 
a certain number of users. Platforms like Google, FB, Insta, use a lot of optimization strategies for each 
type of operation.

Main strategies are:
1. Keep web tier stateless
2. Build redundancy at every tier.
3. Cache data as much as possible
4. Support multiple data centers
5. Host static assets in CDNs
6. Scale data tier by sharding
7. Split data tiers into individual services
8. Monitor systems and use automation tools.


Create back of the envelope estimations to have a rough idea of systems. This will be the major thing 
expected in interviews.


Memory is fast but disk is slow 
Avoid disk seek operations if possible 
Simple compression algorithms are fast 
Compress data before sending it over the internet if possible
Data centers are usually in different regions and transferring data and taking data is expensive

Back of the envelope estimation is all about the process. Simply estimate the number of 
requests, data size, things you need to store or cache, among other things 

Real world system design is extremely complicated, the interviews only try to simulate a meeting discussion

The final design is less important than the work you put in the process
Collaborate, work well under pressure and resolve ambiguity constructively.Ask good questions 

Overengineering is a real disease. Always ask questions and don't focus on answers. Focus on the process.

One of the most important skills in an engineer is to ask the important questions, understand and deal 
with ambiguity and clarify requirements.
If you are making your assumptions instead of getting data from the interviewer, right down the assumptions 


First propose high level design, and get buy-in i.e. the interviewer should agree with your design

Once you go into detail, keep collaborating with the interviewer. Give potential drawbacks of the design,
discuss bottlenecks, and always give potential cases where it can fail. No design is perfect and since 
its all dependent on use cases, the imperfections need to be clear as well.

Discuss rate limiters, all apis use some form of rate limits.

Rate limiters->

1.Acurately limit excessive requests 
2. Low latency-> A rate limiter shouldn't slow down HTTP response time
3. Use as little memory as possible 
4. Distributed-> Can be shared across multiple servers or processes.
5. Exception handling-> Show clear exceptions to users when their requests are throttled.
6. High fault tolerance

We can implement a rate limiter either on the client side or server side 
Generally speaking, client side is unreliable to enforce since client requests can be easily forged 

We can create a rate limiter middleware, which throttles requests to API 
All_Local_Code_and_Notes\System Design images\3rd.png

Cloud microservices have become popular and the rate limiting is usually implemented in an API gateway 
API Gateway is a fully managed service that supports rate limiting, SSL termination, authentication,
IP whitelisting, servicing static content, etc.

Evaluate your current programming language, cache service, etc. Make sure your programming language is
efficient to support rate limiting on server side.

Identify the rate limiting algorithm that fits your business needs. when you implement everything on the server 
side you have full control of the algorithm, but it may change if you are using a third party gateway 


Algorithms for rate limiting:
1.Token bucket
2. Leaking bucket 
3. Fixed window counter 
4. Sliding window log
5. Sliding window counter 

All_Local_Code_and_Notes\System Design images\4th.png -> Token bucket
All_Local_Code_and_Notes\System Design images\5.png->    "

A token bucket algo takes 2 params-> Bucket size and refill rate 

It is usually necessary to have different buckets for different endpoints 

The params might be difficult to tune properly.

Leaking bucket algorithm->
All_Local_Code_and_Notes\System Design images\6.png

It is similar to a token bucket, except the requests are processed at a fixed rate
Takes 2 params-> Bucket size and outflow rate 

Cons-> A burst of traffic fills up the queue with old requests, and if they are not processed 
in time,recent requests will be rate limited.

Fixed window counter algorithm->

The algorithm divides the timeline into fixed sized time windows and assign a counter for each window 

Each request increments the counter by one 

Once the counter reaches the predefined threshold, new requests are dropped until a new time window starts 
All_Local_Code_and_Notes\System Design images\7.png


The con for this is that a spike in traffic at the edges of a window, could cause more requests than the  
allowed quota to go through

All_Local_Code_and_Notes\System Design images\8.png
Here the limit is 5, but in a 1 minute time frame 10 can go through


Sliding window log algorithm->

This fixes the issue of fixed window counter.

The algorithm keeps track of request timestamps. The data is kept in cache in a sorted order like Redis 

When a new request comes in, remove all the outdated timestamps. Add timestamp of the new request to the log 

If the log size is same or lower than the allowed count, the request is accepted or it is rejected.
All_Local_Code_and_Notes\System Design images\9.png



Cons-> The algorithm can consume a lot of memory because even if a request is rejected, its timestamp
might still be stored in the memory.

Sliding window counter algorithm->

Hybrid approach that combines the fixed window counter and a sliding window log.

All_Local_Code_and_Notes\System Design images\10.png

Number of requests in the rolling window is calculated

Requests in current window+ (Requests in previous window * overlap percentage of rolling window and previous window)

62